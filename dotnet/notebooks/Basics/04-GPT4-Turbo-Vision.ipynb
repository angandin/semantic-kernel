{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "#r \"nuget: Azure.AI.OpenAI, 1.0.0-beta.12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "#!import ./../config/Settings.cs\n",
    "\n",
    "using System;\n",
    "using Azure;\n",
    "using Azure.AI.OpenAI;\n",
    "using System.Net.Http;\n",
    "using Newtonsoft.Json;\n",
    "\n",
    "// Configure AI backend used by the kernel\n",
    "var (useAzureOpenAI, model, azureEndpoint, apiKey, orgId) = Settings.LoadFromFile(\"./../config/settings.json\");\n",
    "\n",
    "string endpoint = azureEndpoint;\n",
    "string key = apiKey;\n",
    "model = \"gpt-4-vision\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "string GPT4V_KEY = key; // Set your key here\n",
    "\n",
    "string QUESTION = \"Please describe the picture\";\n",
    "string IMAGE_PATH = \"./Lavaredo.jpeg\"; // Set your image path here\n",
    "\n",
    "//string QUESTION = \"How many steps are needed to complete the entire process?\";\n",
    "//string IMAGE_PATH = \"./flow.png\"; // Set your image path here\n",
    "\n",
    "string GPT4V_ENDPOINT  = azureEndpoint + @\"/openai/deployments/\" + model + \"/chat/completions?api-version=2023-07-01-preview\";\n",
    "\n",
    "var encodedImage = Convert.ToBase64String(File.ReadAllBytes(IMAGE_PATH));\n",
    "using (var httpClient = new HttpClient())\n",
    "{\n",
    "    httpClient.DefaultRequestHeaders.Add(\"api-key\", GPT4V_KEY);\n",
    "    var payload = new\n",
    "    {\n",
    "        messages = new object[]\n",
    "        {\n",
    "            new {\n",
    "                role = \"system\",\n",
    "                content = new object[] {\n",
    "                    new {\n",
    "                        type = \"text\",\n",
    "                        text = \"You are an AI assistant that helps people find information.\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            new {\n",
    "                role = \"user\",\n",
    "                content = new object[] {\n",
    "                    new {\n",
    "                        type = \"image_url\",\n",
    "                        image_url = new {\n",
    "                            url = $\"data:image/jpeg;base64,{encodedImage}\"\n",
    "                        }\n",
    "                    },\n",
    "                    new {\n",
    "                        type = \"text\",\n",
    "                        text = QUESTION\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        temperature = 0.7,\n",
    "        top_p = 0.95,\n",
    "        max_tokens = 800,\n",
    "        stream = false\n",
    "    };\n",
    "\n",
    "    var response = await httpClient.PostAsync(GPT4V_ENDPOINT , new StringContent(JsonConvert.SerializeObject(payload), Encoding.UTF8, \"application/json\"));\n",
    "\n",
    "    if (response.IsSuccessStatusCode)\n",
    "    {\n",
    "        var responseData = JsonConvert.DeserializeObject<dynamic>(await response.Content.ReadAsStringAsync());\n",
    "        //Console.WriteLine(responseData);\n",
    "        Console.WriteLine(responseData.choices[0].message.content);\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        Console.WriteLine($\"Error: {response.StatusCode}, {response.ReasonPhrase}\");\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "// const string rawImageUri = \"https://upload.wikimedia.org/wikipedia/commons/8/8c/Drei_Zinnen-Tre_Cime_Di_Lavaredo_6.JPG\";\n",
    "\n",
    "// OpenAIClient client = new(new Uri(endpoint), new AzureKeyCredential(key));\n",
    "\n",
    "// ChatCompletionsOptions chatCompletionsOptions = new()\n",
    "// {\n",
    "//     DeploymentName = model,\n",
    "//     Messages =\n",
    "//     {\n",
    "//         new ChatRequestSystemMessage(\"You are a helpful assistant that describes images.\"),\n",
    "//         new ChatRequestUserMessage(\n",
    "//             new ChatMessageTextContentItem(\"Hi! Please describe this image\"),\n",
    "//             new ChatMessageImageContentItem(new Uri(rawImageUri))),\n",
    "//     },\n",
    "// };\n",
    "\n",
    "// Response<ChatCompletions> chatResponse = await client.GetChatCompletionsAsync(chatCompletionsOptions);\n",
    "// ChatChoice choice = chatResponse.Value.Choices[0];\n",
    "// if (choice.FinishDetails is StopFinishDetails stopDetails)\n",
    "// {\n",
    "//     Console.WriteLine($\"{choice.Message.Role}: {choice.Message.Content}\");\n",
    "// }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
